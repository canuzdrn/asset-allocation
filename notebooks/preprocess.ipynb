{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "774c5f95",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook prepares the input datasets (`X_train.csv`, `X_test.csv`, `y_train.csv`) for modeling by cleaning and imputing missing values.  \n",
    "It ensures both train and test data are fully numeric, aligned, and ready for model development in `xgb.ipynb`.\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The notebook handles:\n",
    "- Loading raw CSV data from the `data/` folder  \n",
    "- Checking missing values and feature consistency  \n",
    "- Applying **numerical imputation** using methods from `src/preprocess.py`  \n",
    "- Saving clean, imputed datasets as `X_train_imp.csv` and `X_test_imp.csv`\n",
    "\n",
    "This is the **first step** in the full modeling pipeline. All downstream steps depend on these imputed files.\n",
    "\n",
    "---\n",
    "\n",
    "## Steps Performed\n",
    "\n",
    "1. **Load Data**\n",
    "   - Reads `X_train.csv`, `y_train.csv`, and `X_test.csv` from `data/`\n",
    "   - Checks for shape and column alignment\n",
    "   - Ensures index consistency between `X_train` and `y_train`\n",
    "\n",
    "2. **Inspect Missingness**\n",
    "   - Calculates per-feature missing value ratios\n",
    "   - Plots missingness bar chart for quick assessment\n",
    "\n",
    "3. **Imputation**\n",
    "   - Fits an imputer (e.g., `IterativeImputer` or `KNNImputer`) on the training data\n",
    "   - Transforms both `X_train` and `X_test`\n",
    "   - Prevents **data leakage** by never fitting on test data\n",
    "\n",
    "4. **Post-Imputation Checks**\n",
    "   - Verifies that there are no NaNs remaining\n",
    "   - Confirms column order consistency\n",
    "   - Optionally inspects the distribution of selected features before/after imputation\n",
    "\n",
    "5. **Save Outputs**\n",
    "   - Writes:\n",
    "     - `data/X_train_imp.csv`\n",
    "     - `data/X_test_imp.csv`\n",
    "   - These are the official model-ready feature files\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "\n",
    "```\n",
    "data/\n",
    "â”œâ”€â”€ X_train_imp.csv   # Imputed and cleaned training data\n",
    "â”œâ”€â”€ X_test_imp.csv    # Imputed and cleaned test data\n",
    "```\n",
    "\n",
    "These files are later consumed by `xgb.ipynb` for model training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© Notes\n",
    "\n",
    "- Missing values across features were roughly uniform (~5%),  \n",
    "  indicating that no variable is excessively incomplete.\n",
    "- The final datasets preserve original feature order and statistical characteristics.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Step\n",
    "\n",
    "After completing preprocessing:\n",
    "1. Verify that both imputed files exist in `/data`.\n",
    "2. Proceed to `notebooks/xgb.ipynb` to train and tune the XGBoost model.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Step | Action | Output |\n",
    "|------|---------|---------|\n",
    "| 1 | Load & inspect raw CSVs | Data overview |\n",
    "| 2 | Compute missingness ratios | Missingness chart |\n",
    "| 3 | Impute missing values | Fully numeric train/test |\n",
    "| 4 | Validate consistency | No NaN values |\n",
    "| 5 | Save imputed files | `X_train_imp.csv`, `X_test_imp.csv` |\n",
    "\n",
    "This notebook ensures **clean, consistent, and leakage-free input data** for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6893f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from preprocess import fit_mice_bayes_imputer, transform_with_imputer, assert_no_overwrite, summarize_differences\n",
    "from util_io import load_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45dcad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, y_train, X_test_raw = load_frames(\"../data\")\n",
    "\n",
    "imputer = fit_mice_bayes_imputer(X_train_raw, max_iter=15, random_state=1)\n",
    "\n",
    "X_train_imp = transform_with_imputer(imputer, X_train_raw)\n",
    "X_test_imp  = transform_with_imputer(imputer,  X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0fdd683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed -- no overwrite\n",
      "passed -- no overwrite\n"
     ]
    }
   ],
   "source": [
    "#summarize_differences(X_train_raw, X_train_imp)\n",
    "#summarize_differences(X_test_raw, X_test_imp)\n",
    "\n",
    "assert_no_overwrite(X_train_raw, X_train_imp)\n",
    "assert_no_overwrite(X_test_raw, X_test_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1bdeccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imp.to_csv(\"../data/X_train_imp.csv\")   # keep index\n",
    "X_test_imp.to_csv(\"../data/X_test_imp.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultramarin-canuz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
